{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b8b21d",
   "metadata": {},
   "source": [
    "### Configurações Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5254cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_postgresl(df, tb_name=None, write_mode='None'):\n",
    "    df.count()\n",
    "    if tb_name is None:\n",
    "        raise Exception('Informe o nome da tabela')\n",
    "    if write_mode is None:\n",
    "        raise Exception('Informe o mode de escrita: append ou overwrite')\n",
    "        \n",
    "    from datetime import datetime\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        df.write.jdbc(url, table=tb_name, mode=write_mode, properties=properties)\n",
    "    except Exception as e:\n",
    "        print(f'Erro: {e}')\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    total_time = str(end_time - start_time)\n",
    "    \n",
    "    return f'Total time: {total_time} and - Total rows: {df.count()} - Total columns: {len(df.columns)}'\n",
    "\n",
    "\n",
    "\n",
    "    # Criando a sessão do Spark\n",
    "spark  = SparkSession.builder \\\n",
    "    .appName(\"Data Analysis\") \\\n",
    "    .config('spark.jars', '/data/IDAF/DATABASECONNECTOR_JAR_FOLDER/postgresql-42.2.18.jar')\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.executor.instances\",\"8\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"96\")\\\n",
    "    .config(\"spark.default.parallelism\",\"96\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "\n",
    "properties = {\n",
    "    \"user\" : \"postgres\",\n",
    "    \"password\" : \"cidacs\",\n",
    "    \"driver\" : \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b3550-eebc-4a29-83de-58c7f8ddf882",
   "metadata": {},
   "source": [
    "### Configurações Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "787acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 300)\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 100)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bdd252",
   "metadata": {},
   "source": [
    "### Lendo dados enriquecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64224f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = (spark\n",
    "            .read\n",
    "            .parquet('/data/IDAF/PROJETOS/PARCERIA_CIDACS_PHDC/banco_original_enriched'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21352a3",
   "metadata": {},
   "source": [
    "### Procedure Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9933edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_procedure_occurrence = Window.partitionBy().orderBy('person_id', 'dtnasc_sinasc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc28f863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49392036"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_procedure_occurrence = (df_input\n",
    "#                           .filter(F.col('id_cidacs_mae_sinasc')==10010232335)\n",
    "                         .withColumn('person_id', F.col('person_id'))\n",
    "                         .withColumn('tpconfirma_mae_calc', F.lit(4193412))\n",
    "                         .withColumn('parto_sinasc_calc', F.when(F.col('parto_sinasc').isin(0,88,99), 0)\n",
    "                                                            .when(F.col('parto_sinasc')==1, 44784097)\n",
    "                                                            .when(F.col('parto_sinasc')==1, 4015701)\n",
    "                                                            .when(F.col('parto_sinasc')==1, 44784097)\n",
    "                                                            .otherwise(0))\n",
    "                         .withColumn('procedure', F.concat_ws(\",\", F.col('tpconfirma_mae_calc'), F.col('parto_sinasc_calc'))) \n",
    "                         .withColumn('procedure_concept_id', F.explode(F.split(F.col('procedure'), ',')))\n",
    "                         .withColumn('procedure_source_concept_id', F.lit(None))\n",
    "                         .withColumn('procedure_type_concept_id', F.lit(32879))\n",
    "                         .withColumn('procedure_date', F.when(F.col('procedure_concept_id')==4193412, F.coalesce(F.col('dt_notific_mae'), F.col('dtnasc_sinasc')))\n",
    "                                     .when(F.col('procedure_concept_id')==44784097, F.col('dtnasc_sinasc'))\n",
    "                                     .when(F.col('procedure_concept_id')==4015701, F.col('dtnasc_sinasc'))\n",
    "                                     .otherwise(F.coalesce(F.col('dt_notific_mae'), F.col('dtnasc_sinasc'))) \n",
    "                                    )\n",
    "                         .withColumn('procedure_source_value', F.when(F.col('procedure_concept_id')==4193412, F.col('tpconfirma_mae')).otherwise(None))\n",
    "                         .withColumn('quantity', F.lit(None))\n",
    "                           .withColumn('procedure_occurrence_id', F.row_number().over(window_procedure_occurrence))\n",
    "                           .withColumn('procedure_datetime', F.lit(None))\n",
    "                           .withColumn('procedure_end_date', F.lit(None))\n",
    "                           .withColumn('procedure_end_datetime', F.lit(None))\n",
    "                           .withColumn('modifier_concept_id', F.lit(None))\n",
    "                           .withColumn('provider_id', F.lit(None))\n",
    "                           .withColumn('visit_occurrence_id', F.col('visit_occurrence_id'))\n",
    "                           .withColumn('visit_detail_id', F.lit(None))\n",
    "                           .withColumn('modifier_source_value', F.lit(None))\n",
    "                          ).select( F.col('procedure_occurrence_id').cast('integer'),\n",
    "                                    F.col('person_id').cast('integer'),\n",
    "                                    F.col('procedure_concept_id').cast('integer'),\n",
    "                                    F.col('procedure_date').cast('date'),\n",
    "                                    F.col('procedure_datetime').cast('timestamp'),\n",
    "                                    F.col('procedure_end_date').cast('date'),\n",
    "                                    F.col('procedure_end_datetime').cast('timestamp'),\n",
    "                                    F.col('procedure_type_concept_id').cast('integer'),\n",
    "                                    F.col('modifier_concept_id').cast('integer'),\n",
    "                                    F.col('quantity').cast('float'),\n",
    "                                    F.col('provider_id').cast('integer'),\n",
    "                                    F.col('visit_occurrence_id').cast('integer'),\n",
    "                                    F.col('visit_detail_id').cast('integer'),\n",
    "                                    F.col('procedure_source_value').cast('string'),\n",
    "                                    F.col('procedure_source_concept_id').cast('integer'),\n",
    "                                    F.col('modifier_source_value').cast('string')\n",
    "                                )\n",
    "df_procedure_occurrence.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e7942",
   "metadata": {},
   "source": [
    "## Salvando dados no Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b01e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/19 15:55:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 15:55:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 15:55:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 15:55:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 15:55:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/03/19 16:49:24 ERROR Executor: Exception in task 0.0 in stage 77.0 (TID 2641)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO procedure_occurrence (\"procedure_occurrence_id\",\"person_id\",\"procedure_concept_id\",\"procedure_date\",\"procedure_datetime\",\"procedure_end_date\",\"procedure_end_datetime\",\"procedure_type_concept_id\",\"modifier_concept_id\",\"quantity\",\"provider_id\",\"visit_occurrence_id\",\"visit_detail_id\",\"procedure_source_value\",\"procedure_source_concept_id\",\"modifier_source_value\") VALUES (1,1,4193412,'2002-07-05 -03'::date,NULL,NULL,NULL,32879,NULL,NULL,NULL,1,NULL,NULL,NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1404)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1429)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:507)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n",
      "\t... 23 more\n",
      "25/03/19 16:49:24 WARN TaskSetManager: Lost task 0.0 in stage 77.0 (TID 2641) (tre2-172-16-1-22.tre2.cidacs executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO procedure_occurrence (\"procedure_occurrence_id\",\"person_id\",\"procedure_concept_id\",\"procedure_date\",\"procedure_datetime\",\"procedure_end_date\",\"procedure_end_datetime\",\"procedure_type_concept_id\",\"modifier_concept_id\",\"quantity\",\"provider_id\",\"visit_occurrence_id\",\"visit_detail_id\",\"procedure_source_value\",\"procedure_source_concept_id\",\"modifier_source_value\") VALUES (1,1,4193412,'2002-07-05 -03'::date,NULL,NULL,NULL,32879,NULL,NULL,NULL,1,NULL,NULL,NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1404)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1429)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:507)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n",
      "\t... 23 more\n",
      "\n",
      "25/03/19 16:49:24 ERROR TaskSetManager: Task 0 in stage 77.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro: An error occurred while calling o1204.jdbc.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 2641) (tre2-172-16-1-22.tre2.cidacs executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO procedure_occurrence (\"procedure_occurrence_id\",\"person_id\",\"procedure_concept_id\",\"procedure_date\",\"procedure_datetime\",\"procedure_end_date\",\"procedure_end_datetime\",\"procedure_type_concept_id\",\"modifier_concept_id\",\"quantity\",\"provider_id\",\"visit_occurrence_id\",\"visit_detail_id\",\"procedure_source_value\",\"procedure_source_concept_id\",\"modifier_source_value\") VALUES (1,1,4193412,'2002-07-05 -03'::date,NULL,NULL,NULL,32879,NULL,NULL,NULL,1,NULL,NULL,NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1404)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1429)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:507)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n",
      "\t... 23 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3516)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4310)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4308)\n",
      "\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3516)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:766)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO procedure_occurrence (\"procedure_occurrence_id\",\"person_id\",\"procedure_concept_id\",\"procedure_date\",\"procedure_datetime\",\"procedure_end_date\",\"procedure_end_datetime\",\"procedure_type_concept_id\",\"modifier_concept_id\",\"quantity\",\"provider_id\",\"visit_occurrence_id\",\"visit_detail_id\",\"procedure_source_value\",\"procedure_source_concept_id\",\"modifier_source_value\") VALUES (1,1,4193412,'2002-07-05 -03'::date,NULL,NULL,NULL,32879,NULL,NULL,NULL,1,NULL,NULL,NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:169)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2286)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1404)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1429)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:507)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:870)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1644)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:748)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:904)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:903)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"xpk_procedure_occurrence\"\n",
      "  Detail: Key (procedure_occurrence_id)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2553)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2285)\n",
      "\t... 23 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Total time: 0:53:44.114409 and - Total rows: 49392036 - Total columns: 16'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_to_postgresl(df_procedure_occurrence, tb_name='procedure_occurrence', write_mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714e4b6",
   "metadata": {},
   "source": [
    "### Deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8976f",
   "metadata": {},
   "source": [
    "### Salvando CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77898bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_procedure_occurrence.repartition(1).write.option(\"quoteAll\",True).csv('/data/IDAF/PROJETOS/PARCERIA_CIDACS_PHDC/omop_tables_revisadas/procedure_occurrence', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beea3f6",
   "metadata": {},
   "source": [
    "## SQL de insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b417f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CREATE TABLE public.procedure_occurrence (\n",
    "# -- \t\t\tprocedure_occurrence_id integer NOT NULL,\n",
    "# -- \t\t\tperson_id integer NOT NULL,\n",
    "# -- \t\t\tprocedure_concept_id integer NOT NULL,\n",
    "# -- \t\t\tprocedure_date date NOT NULL,\n",
    "# -- \t\t\tprocedure_datetime TIMESTAMP NULL,\n",
    "# -- \t\t\tprocedure_end_date date NULL,\n",
    "# -- \t\t\tprocedure_end_datetime TIMESTAMP NULL,\n",
    "# -- \t\t\tprocedure_type_concept_id integer NOT NULL,\n",
    "# -- \t\t\tmodifier_concept_id integer NULL,\n",
    "# -- \t\t\tquantity integer NULL,\n",
    "# -- \t\t\tprovider_id integer NULL,\n",
    "# -- \t\t\tvisit_occurrence_id integer NULL,\n",
    "# -- \t\t\tvisit_detail_id integer NULL,\n",
    "# -- \t\t\tprocedure_source_value varchar(50) NULL,\n",
    "# -- \t\t\tprocedure_source_concept_id integer NULL,\n",
    "# -- \t\t\tmodifier_source_value varchar(50) NULL );\n",
    "\n",
    "\n",
    "# -- CREATE TABLE public.procedure_occurrence_pyspark (\n",
    "# -- \t\t\tprocedure_occurrence_id varchar,\n",
    "# -- \t\t\tperson_id varchar,\n",
    "# -- \t\t\tprocedure_concept_id varchar,\n",
    "# -- \t\t\tprocedure_date varchar,\n",
    "# -- \t\t\tprocedure_datetime varchar,\n",
    "# -- \t\t\tprocedure_end_date varchar,\n",
    "# -- \t\t\tprocedure_end_datetime varchar,\n",
    "# -- \t\t\tprocedure_type_concept_id varchar,\n",
    "# -- \t\t\tmodifier_concept_id varchar,\n",
    "# -- \t\t\tquantity varchar,\n",
    "# -- \t\t\tprovider_id varchar,\n",
    "# -- \t\t\tvisit_occurrence_id varchar,\n",
    "# -- \t\t\tvisit_detail_id varchar,\n",
    "# -- \t\t\tprocedure_source_value varchar,\n",
    "# -- \t\t\tprocedure_source_concept_id varchar,\n",
    "# -- \t\t\tmodifier_source_value  varchar);\n",
    "\n",
    "\t\t\t\t\t\t\t\t   \n",
    "# insert into public.procedure_occurrence (\n",
    "# \t\t\tprocedure_occurrence_id,\n",
    "# \t\t\tperson_id,\n",
    "# \t\t\tprocedure_concept_id,\n",
    "# \t\t\tprocedure_date,\n",
    "# \t\t\tprocedure_datetime,\n",
    "# \t\t\tprocedure_end_date,\n",
    "# \t\t\tprocedure_end_datetime,\n",
    "# \t\t\tprocedure_type_concept_id,\n",
    "# \t\t\tmodifier_concept_id,\n",
    "# \t\t\tquantity,\n",
    "# \t\t\tprovider_id,\n",
    "# \t\t\tvisit_occurrence_id,\n",
    "# \t\t\tvisit_detail_id,\n",
    "# \t\t\tprocedure_source_value,\n",
    "# \t\t\tprocedure_source_concept_id,\n",
    "# \t\t\tmodifier_source_value\n",
    "# )\n",
    "# SELECT \n",
    "# \t\t\tcast(procedure_occurrence_id as integer),\n",
    "# \t\t\tcast(person_id as integer),\n",
    "# \t\t\tcast(case when procedure_concept_id = '' then null else procedure_concept_id end as integer),\n",
    "# \t\t\tcast(procedure_date as date),\n",
    "# \t\t\tcast(case when procedure_datetime = '' then null else procedure_datetime end as timestamp),\n",
    "# \t\t\tcast(case when procedure_end_date = '' then null else procedure_end_date end as date),\n",
    "# \t\t\tcast(case when procedure_end_datetime = '' then null else procedure_end_datetime end as timestamp),\n",
    "# \t\t\tcast(case when procedure_type_concept_id = '' then null else procedure_type_concept_id end as integer),\n",
    "# \t\t\tcast(case when modifier_concept_id ='' then null else modifier_concept_id end as integer),\n",
    "# \t\t\tcast(case when quantity = '' then null else quantity end as integer),\n",
    "# \t\t\tcast(case when provider_id ='' then null else provider_id end as integer),\n",
    "# \t\t\tcast(case when visit_occurrence_id = '' then null else visit_occurrence_id end as integer),\n",
    "# \t\t\tcast(case when visit_detail_id = '' then null else visit_detail_id end as integer),\n",
    "# \t\t\tcast(procedure_source_value as varchar),\n",
    "# \t\t\tcast(case when procedure_source_concept_id = '' then null else procedure_source_concept_id end as integer),\n",
    "# \t\t\tcast(modifier_source_value as varchar)\n",
    "# \tFROM public.procedure_occurrence_pyspark;\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
